import numpy as np
import pickle
import glob
import os
# å®šä¹‰è¦åˆå¹¶çš„ .npz æ–‡ä»¶è·¯å¾„
from concurrent.futures import ThreadPoolExecutor

# ori_folder = "/ssd1/lishujia/chois_release/mesh_cond_data1/train"
# target_folder = "/ssd1/lishujia/chois_release/mesh_cond_data2/train"
# output_file = os.path.join(target_folder, "merged_data")
# # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸æ¥å­˜å‚¨æ‰€æœ‰æ•°æ®
# def load_npz(file_path):
#     """ é€æ­¥åŠ è½½ .npz å¹¶å†™å…¥ pickle """
#     index = int(os.path.basename(file_path).split('.')[0])
#     print(index)
#     with np.load(file_path, allow_pickle=True) as data:
#         return index, {key: data[key] for key in data.files}

# npz_files = [os.path.join(ori_folder, file) for file in os.listdir(ori_folder) if file.endswith(".npz")]

# batch_size = 1000  # æ¯ä¸ª .p æ–‡ä»¶æœ€å¤šå­˜ 1000 ä¸ªæ•°æ®
# file_count = 0
# merged_data = {}

# with ThreadPoolExecutor(max_workers=16) as executor:
#     for i, (index, data) in enumerate(executor.map(load_npz, npz_files)):
#         merged_data[index] = data
#         if (i + 1) % batch_size == 0:
#             with open(f"{output_file}_{file_count}.p", 'wb') as f:
#                 pickle.dump(merged_data, f)
#             merged_data = {}  # æ¸…ç©ºï¼Œå¼€å§‹æ–°çš„ batch
#             file_count += 1

# # **æœ€åä¸€æ‰¹**
# if merged_data:
#     with open(f"{output_file}_{file_count}.p", 'wb') as f:
#         pickle.dump(merged_data, f)

# input_folder = "/ssd1/lishujia/chois_release/mesh_cond_data2/train"  # å­˜æ”¾ .p æ–‡ä»¶çš„æ–‡ä»¶å¤¹
# output_file = os.path.join(input_folder, "final_merged_data.p")  # æœ€ç»ˆåˆå¹¶åçš„æ–‡ä»¶

# merged_data = {}

# # è·å–æ‰€æœ‰ .p æ–‡ä»¶ï¼Œå¹¶æŒ‰æ–‡ä»¶åæ’åºï¼ˆé˜²æ­¢ä¹±åºï¼‰
# p_files = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(".p")])

# # éå†æ‰€æœ‰ .p æ–‡ä»¶ï¼Œé€ä¸ªåŠ è½½å¹¶åˆå¹¶
# for p_file in p_files:
#     with open(p_file, 'rb') as f:
#         data = pickle.load(f)
#         merged_data.update(data)  # **åˆå¹¶å­—å…¸**

#     print(f"Loaded {p_file}, total keys: {len(merged_data)}")  # æ˜¾ç¤ºå½“å‰å·²åˆå¹¶çš„ key æ•°é‡

# # **ä¿å­˜æœ€ç»ˆåˆå¹¶çš„å­—å…¸**
# with open(output_file, 'wb') as f:
#     pickle.dump(merged_data, f)
import os
import numpy as np
import multiprocessing



def sample_point_cloud(point_cloud, target_points):
    """
    è¿œç‚¹é‡‡æ ·ç‚¹äº‘ï¼ˆå¦‚æœæ•°æ®è¾ƒå¤§ï¼Œå»ºè®®ç”¨ PyTorch å®ç°æ›´å¿«çš„ FPSï¼‰ã€‚
    
    Args:
        point_cloud: np.ndarray, å½¢çŠ¶ä¸º [frame, num_points, 3]
        target_points: int, ç›®æ ‡ç‚¹æ•°
        
    Returns:
        sampled_point_cloud: np.ndarray, å½¢çŠ¶ä¸º [frame, target_points, 3]
    """
    frame, num_points, _ = point_cloud.shape
    sampled_indices = np.random.choice(num_points, target_points, replace=False)  # âœ… ä¿®æ­£ axis é”™è¯¯
    return point_cloud[:, sampled_indices, :]

def process_file(file_path, output_folder):
    """
    å¤„ç†å•ä¸ª .npz æ–‡ä»¶ï¼š
    1. è¯»å– human_mesh å’Œ obj_mesh
    2. åˆ†åˆ«é‡‡æ · 2000 å’Œ 1000 ç‚¹
    3. åœ¨ axis=1 æ–¹å‘æ‹¼æ¥
    4. ä»¥ .npy æ ¼å¼ä¿å­˜åˆ°ç›®æ ‡æ–‡ä»¶å¤¹

    Args:
        file_path: str, è¾“å…¥ .npz æ–‡ä»¶è·¯å¾„
        output_folder: str, è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    """
    # try:
    with np.load(file_path, allow_pickle=True) as data:
        human_mesh = data['human_mesh']  # å½¢çŠ¶ [frame, 3000, 3]
        obj_mesh = data['obj_mesh']  # å½¢çŠ¶ [frame, 1500, 3]

            # é‡‡æ ·
        sampled_human_mesh = sample_point_cloud(human_mesh, target_points=2000)  # [frame, 2000, 3]
        sampled_obj_mesh = sample_point_cloud(obj_mesh, target_points=1000)  # [frame, 1000, 3]

            # åˆå¹¶
        merged_mesh = np.concatenate((sampled_human_mesh, sampled_obj_mesh), axis=1)  # [frame, 3000, 3]

            # ä¿å­˜ä¸º .npy
        file_name = os.path.splitext(os.path.basename(file_path))[0] + ".npy"
        output_path = os.path.join(output_folder, file_name)
        np.save(output_path, merged_mesh)

    print(f"âœ… Processed: {file_name}")
    # except Exception as e:
    #     print(f"âŒ Error processing {file_path}: {e}")

def process_folder(input_folder, output_folder, num_workers=8):
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„ .npz æ–‡ä»¶ï¼Œå¹¶ä¿å­˜ä¸º .npy æ–‡ä»¶ã€‚

    Args:
        input_folder: str, è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
        output_folder: str, è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        num_workers: int, è¿›ç¨‹æ•°é‡
    """
    os.makedirs(output_folder, exist_ok=True)  # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹

    # è·å–æ‰€æœ‰ .npz æ–‡ä»¶
    npz_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.npz')]

    # å¤šè¿›ç¨‹å¤„ç†
    with multiprocessing.Pool(processes=num_workers) as pool:
        pool.starmap(process_file, [(file, output_folder) for file in npz_files])




if __name__ == "__main__":

    input_folder = "/ssd1/lishujia/chois_release/mesh_cond_data1/train"  # ä¿®æ”¹ä¸ºä½ çš„ .npz æ–‡ä»¶å¤¹è·¯å¾„
    output_folder = "/ssd1/lishujia/chois_release/mesh_cond_data2/train"  # ä¿®æ”¹ä¸ºä½ çš„è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    # data1 = np.load('/ssd1/lishujia/chois_release/mesh_cond_data1/test/00000.npz')
    # data2 = np.load('/ssd1/lishujia/chois_release/mesh_cond_data2/train/00000.npy')
    # import pdb; pdb.set_trace()
    process_folder(input_folder, output_folder, num_workers=12)
    print("ğŸ‰ æ‰€æœ‰ .npz æ–‡ä»¶å·²å¤„ç†å¹¶ä¿å­˜ä¸º .npyï¼")
