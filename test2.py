import numpy as np
import pickle
import glob
import os
# å®šä¹‰è¦åˆå¹¶çš„ .npz æ–‡ä»¶è·¯å¾„
from concurrent.futures import ThreadPoolExecutor

# ori_folder = "/ssd1/lishujia/chois_release/mesh_cond_data1/train"
# target_folder = "/ssd1/lishujia/chois_release/mesh_cond_data2/train"
# output_file = os.path.join(target_folder, "merged_data")
# # åˆ›å»ºä¸€ä¸ªç©ºå­—å…¸æ¥å­˜å‚¨æ‰€æœ‰æ•°æ®
# def load_npz(file_path):
#     """ é€æ­¥åŠ è½½ .npz å¹¶å†™å…¥ pickle """
#     index = int(os.path.basename(file_path).split('.')[0])
#     print(index)
#     with np.load(file_path, allow_pickle=True) as data:
#         return index, {key: data[key] for key in data.files}

# npz_files = [os.path.join(ori_folder, file) for file in os.listdir(ori_folder) if file.endswith(".npz")]

# batch_size = 1000  # æ¯ä¸ª .p æ–‡ä»¶æœ€å¤šå­˜ 1000 ä¸ªæ•°æ®
# file_count = 0
# merged_data = {}

# with ThreadPoolExecutor(max_workers=16) as executor:
#     for i, (index, data) in enumerate(executor.map(load_npz, npz_files)):
#         merged_data[index] = data
#         if (i + 1) % batch_size == 0:
#             with open(f"{output_file}_{file_count}.p", 'wb') as f:
#                 pickle.dump(merged_data, f)
#             merged_data = {}  # æ¸…ç©ºï¼Œå¼€å§‹æ–°çš„ batch
#             file_count += 1

# # **æœ€åä¸€æ‰¹**
# if merged_data:
#     with open(f"{output_file}_{file_count}.p", 'wb') as f:
#         pickle.dump(merged_data, f)

# input_folder = "/ssd1/lishujia/chois_release/mesh_cond_data2/train"  # å­˜æ”¾ .p æ–‡ä»¶çš„æ–‡ä»¶å¤¹
# output_file = os.path.join(input_folder, "final_merged_data.p")  # æœ€ç»ˆåˆå¹¶åçš„æ–‡ä»¶

# merged_data = {}

# # è·å–æ‰€æœ‰ .p æ–‡ä»¶ï¼Œå¹¶æŒ‰æ–‡ä»¶åæ’åºï¼ˆé˜²æ­¢ä¹±åºï¼‰
# p_files = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(".p")])

# # éå†æ‰€æœ‰ .p æ–‡ä»¶ï¼Œé€ä¸ªåŠ è½½å¹¶åˆå¹¶
# for p_file in p_files:
#     with open(p_file, 'rb') as f:
#         data = pickle.load(f)
#         merged_data.update(data)  # **åˆå¹¶å­—å…¸**

#     print(f"Loaded {p_file}, total keys: {len(merged_data)}")  # æ˜¾ç¤ºå½“å‰å·²åˆå¹¶çš„ key æ•°é‡

# # **ä¿å­˜æœ€ç»ˆåˆå¹¶çš„å­—å…¸**
# with open(output_file, 'wb') as f:
#     pickle.dump(merged_data, f)
import os
import numpy as np
import multiprocessing

import os
import numpy as np
import pickle
import multiprocessing

def load_npy(file_path):
    """
    è¯»å– .npy æ–‡ä»¶å¹¶è¿”å› (é”®, æ•°æ®) å¯¹ã€‚
    
    Args:
        file_path: str, .npy æ–‡ä»¶è·¯å¾„
        
    Returns:
        tuple: (int, np.ndarray) - æ–‡ä»¶åçš„æ•°å€¼éƒ¨åˆ†ä½œä¸ºé”®ï¼Œæ•°ç»„ä½œä¸ºå€¼
    """
    try:
        key = int(os.path.basename(file_path).split('.')[0])  # æå–æ–‡ä»¶åå‰çš„æ•°å­—
        print(key)
        data = np.load(file_path, allow_pickle=True)  # è¯»å– .npy æ–‡ä»¶
        return key, data
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        #return None  # é‡åˆ°é”™è¯¯è¿”å› Noneï¼Œåç»­ä¼šè¿‡æ»¤æ‰

def save_dict_to_pickle(data_dict, output_file):
    """
    ä¿å­˜å­—å…¸åˆ° .p (pickle) æ–‡ä»¶ã€‚
    
    Args:
        data_dict: dict, éœ€è¦ä¿å­˜çš„å­—å…¸
        output_file: str, è¾“å‡º .p æ–‡ä»¶è·¯å¾„
    """
    with open(output_file, 'wb') as f:
        pickle.dump(data_dict, f)
    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_file}")

def process_folder(input_folder, output_file, num_workers=8):
    """
    è¯»å–ç›®å½•ä¸‹çš„æ‰€æœ‰ .npy æ–‡ä»¶ï¼Œå¹¶ä¿å­˜ä¸º .p æ–‡ä»¶ã€‚
    
    Args: 
        input_folder: str, è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
        output_file: str, è¾“å‡º .p æ–‡ä»¶è·¯å¾„
        num_workers: int, è¿›ç¨‹æ•°é‡
    """
    # è·å–æ‰€æœ‰ .npy æ–‡ä»¶
    npy_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.npy')]

    # å¤šè¿›ç¨‹è¯»å– .npy æ–‡ä»¶
    with multiprocessing.Pool(processes=num_workers) as pool:
        results = pool.map(load_npy, npy_files)

    # è¿‡æ»¤æ‰åŠ è½½å¤±è´¥çš„é¡¹ï¼ˆNoneï¼‰
    data_dict = {k: v for k, v in results if k is not None}

    # ä¿å­˜åˆ° .p æ–‡ä»¶
    save_dict_to_pickle(data_dict, output_file)


if __name__ == "__main__":
    #data = np.load('/ssd1/lishujia/chois_release/mesh_cond_data2/test/00002.npz')
    #data1 = np.load('/ssd1/lishujia/chois_release/mesh_cond_data1/test/00002.npz')
    input_folder = "/ssd1/lishujia/chois_release/mesh_cond_data2/train"  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    output_file = "/ssd1/lishujia/chois_release/mesh_cond_data2/train_data.p"  # æ›¿æ¢ä¸ºä½ çš„è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
 
    process_folder(input_folder, output_file, num_workers=12)
    print("ğŸ‰ æ‰€æœ‰ .npy æ–‡ä»¶å·²å¤„ç†å®Œæ¯•å¹¶ä¿å­˜ï¼")